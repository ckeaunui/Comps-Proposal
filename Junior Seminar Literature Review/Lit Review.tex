\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{wrapfig}
\begin{document}

\title{Comp 390: Literature Review}
\author{\footnotesize Christian Keaunui \\
\IEEEauthorblock{
\textit{\footnotesize Occidental College} \\
\textit{\footnotesize Computer Science Department} \\
\footnotesize Los Angeles, CA USA \\
\footnotesize ckeaunui@oxy.edu}
}
\date{May 2022}
\maketitle


\section*{\centering{Problem context}}
\bigskip

\subsection*{Importance}

Of all the fields of computer science I have been introduced to, I find machine learning to be the most interesting.  I believe it will be fundamental in most jobs in the near future, and being an AI software developer is my current dream job.  However my experience is very shallow and I want to use my comps as a way to further explore the subject.  This project will consist of research across multiple different machine learning algorithms to determine which one is the most applicable, and then creating that algorithm from scratch to learn the fundamentals of how the model works.  The purpose of the code is to play Tetris at a high level, but the project as a whole will give me insight into what being an AI developer is like.  Through this project I hope to determine if I am as passionate about AI as I believe I am.


\subsection*{Relevant Information and Nuances}

\begin{wrapfigure}{r}{118}
\centering
\includegraphics[scale=0.09]{Wall-Kick.png}
\caption{}
\end{wrapfigure}

Most people know the rules of Tetris already so I wont go into detail explaining them, but essentially the player is given a random piece and has to decide where to put it.  Each time a row is filled it is deleted and clears up space, so the more rows the player can clear the longer they will survive.  

Understanding how the Tetris scoring system functions is also vital for this project.  Every line cleared awards points, but clearing multiple lines at once gives bonus scorer based on the number of lines cleared. This means that the player is awarded for stacking pieces up high and then clearing them all at once, although this is a more dangerous strategy as well.

One of the nuances of this project is all the different versions of Tetris that exist.  Currently over a dozen different versions of Tetris have been made, and while the rules haven’t changed the mechanics certainly have.  For example, one area that differs between versions is the rotation of the pieces.  In some versions an illegal rotation cannot be made, while in other versions there is more leniency allowing for greater piece mobility.  An example of one of these rotary mechanics can be see figure 1. Since the long piece is against the right wall, rotating the piece would push one of the tiles outside the frame because the axis of rotation is in the center.  Older versions of Tetris do not allow a rotation in this position, but newer ones will rotate the piece and then shift it right by a tile to keep the piece on the board.  This mechanic is called a 'wall kick'


\begin{wrapfigure}{r}{0}
\includegraphics[scale=0.35]{Long Maneuver.png}
\caption{}
\end{wrapfigure}


Some versions also manipulate the rotation by skipping illegal rotations, and jumping straight to the next legal one. This allows for maneuvers such as the one seen in figure 2 to exist.  In this example, the blue piece abused the rotation mechanics to fit into a spot that would be inaccessible in many other versions.  After dropping the piece into the first vertical hole, a right rotation is not legal, but if the piece could rotate right 3 times it would be legal. In this version, trying to rotate the piece once to the right will skip the two illegal states and return the third rotation where the piece is lying on its side.  Repeating this maneuver would allows the user to reach this interesting position

There are many other examples of rotation mechanics changing based on the version of Tetris, with some newer versions even reverting to older mechanics.  This means that the nuances of the mechanics will change drastically depending on the version of Tetris being played. 

Aside from inconsistent mechanics, another important thing for the reader to understand is the size of Tetris data for state based searching.  On a blank board a piece can have 9 - 34 possible end locations depending on the piece. The square piece has no rotations which limits the total number of states it can be in, while pieces such as the 'L' shape have all four rotations being unique.  Giving them more possibilities.  On non-blank boards the board data is also needed to consider possible states.  There are 10 columns and 21 rows, and since each tile is either filled or not there are a total of 2\textsuperscript{210} different board possibilities.  This figure includes illegal boards such as a piece floating in the middle of the screen, but even if they could be removed there is still an incredibly large amount of data to work with.


\bigskip
\section*{\centering{Technical Background}}
\bigskip


\subsection*{Technical Terms}
Throughout this project many terms related to both the models algorithm and the Tetris game itself will be used. Below are definitions for each term and short descriptions of their usefulness.


\subsubsection*{Tetris Terms}

A "Tetris" is when four rows of a board are cleared at one time.  This is only possible when a blue 4x1 piece is placed vertically into a one-wide hole

A "Tetrimio" is an individual Tetris piece. All pieces are Tetrimios so the term does not reference any one particular shape

A "Wall Kick" is when the piece is bumped horizontally from rotating into an adjacent piece.  For example, if a 4x1 piece is vertically aligned against the far left wall, rotating it would move some of the tiles off the board (since the rotation axis is in the center of the piece).  To prevent this we shift the blue piece to the right until no pieces are off the board.  

A "Hard Drop" is when the piece is dropped all the way down in its current position by pressing the space bar.  This places the piece in the gray area it outlines on the board. 

A "Soft Drop" is when the piece is lowered by one row by clicking the down key. Soft dropping awards one point to the score per line dropped


\subsubsection*{Model/Algorithm Terms}

An "Agent" is the current entity which represents the model. The agent will be the bot that trains to play the game.

An "Environment" is the world that the agent is put in, with some scenario that the model has to overcome.

A "Reward" is a short-term return value instantly given to the agent when they perform certain tasks. The values awarded range from +1 for a favorable task and -1 for unfavorable tasks.  

A "State" is the current cohesive description of the environments components.  For Tetris it will compose of the boards current tile distribution (coordinates of free and occupied tiles), the current active piece, the stored piece, a Boolean representing if the active piece can be stored, and the next upcoming piece(s).  

A "Value" is the expected long-term reward that the agent will receive, which the computer compares with its short-term reward.

A "Value Function" is a function which specifies the reward an agent will get from a given state. This combines short and long term rewards

A "Q-Value" is similar to a value but also requires the current action. The value determines how good an action is overall while the q-value determines how good an action is in the environments current state.  Q-values are represented by Q(S, A) where S is the current state and A is the action.  The resulting Q-value represents the agents estimation for how good A is given S.  


\subsection*{Background}


This project will use a deep learning algorithm very similar to that used by the DeepMind team in article X.  This model uses a neural network which is designed to approximate the following function:  

\includegraphics[scale=0.3]{NeuralNetAlgorithm1.png}

This functions goal is to select actions which maximizes its own future rewards. The term Q*(s,a) represents the optimal action-value function given an action 'a' and a state 's' at time 't'.  Pi is a policy mapping sequence to actions, which determines which known action is best suited for the environments current state.  This same process is then repeated for every sub-sequential state. Finally, Y is a discount factor which is used to reduce the reward value with respect to 't'.

The optimal action-value function follows the Bellman equations identity which is based on the following process: If for each possible state we know the optimal reward value of all actions, the best choice would be the action which results in the maximum value for r + yQ*(s\textsuperscript{1}, a\textsuperscript{1}).  Thus, 

\includegraphics[scale=0.45]{NeuralNetMax.png}

Essentially what this function does is take a boards current state and assigns an immediate reward value to each actions resulting state.  This is a base reward which is the agents temporary indicator of how good that state is.  This estimation will be the average values of all potential paths from the current state.  As each sub-sequent path is explored and assigned more accurate values, the algorithm adjusts the values of previously assigned states.  This makes early state values dynamic, resulting in a wider range of explored paths.  

Each array will then be passed into the neural network for training.  This training will follow a greedy-Epsilon model with a randomness coefficient of 0.05 - 0.10. Adding randomness gives the agent a chance to explore a non-optimal path and search for unexpected results.  This increases the chance at discovering a new best solution because it increases the amount of paths explored.  

90-95\% of the time the computer will pick the path it currently believes to be optimal rather than picking a random path.  Each time it picks a path to explore, that decision is stored in a separate file containing all the computers choices.  This allows the computer to see how it has changed its priorities over time as it explores the states further.  By seeing past decisions the computer can recalculate a states value to make it more accurate if it has not been adjusted recently.  As this file grows there will be too much data to read through, so a sample of it will be used instead for training. This will assist in the training because it will give an assortment of state-action pairs from different times during training, allowing the agent to train on results that may not be considered optimal currently.  This method is called an experience replay.  As the experience replay grows the model should get more accurate because it has more defined values for states.  The bot determines the current states value by estimating the values of each consecutive possible state.  As these state values are defined, the amount of estimating the model has to do is reduced.  This is because it can see the values assigned to previous states in the experience replay, and then calculate the current state with these previously recorded values rather than estimations.


\bigskip
\section*{\centering{Prior Work}}
\bigskip


\subsection*{DeepMind Technologies: Playing Atari with Deep Reinforcement Learning \\
\footnotesize\normalfont https://arxiv.org/pdf/1312.5602.pdf}
\medskip

This is an article from 2013 where a team of 7 DeepMind programmers used a deep learning model to play Atari. At the time this was a revolutionary development for machine learning, and the model was even able to beat an Atari professional in 3/7 different Atari 2600 games without any retraining required.  According to the article, this was the first time a deep learning model was able to "successfully learn control policies directly from high-dimensional sensory input using reinforcement leaning" (pg 1).  This means that all the computers input data was based off human senses, in this case vision.  This project has a very similar structure to my Tetris AI and the methods used in this paper heavily influenced my intended approach.

One key component our projects will have in common is experience replays.  This stores an agents current trained state for future use after its original policy is forgotten. This allows the agent to store multiple different iterations of itself throughout the training process, allowing further training to be done at any state.  This will be useful if an agents training starts going poorly and needs to be restored to an old state.  This allows the user to assist in the models training which will theoretically result in a better performing agent

Another step of my project influenced by this article is the use of the optimal action-value function.  This follows the intuition of the Bellman Equation (See figure). 


\includegraphics[scale=0.6]{Optimal action-value function.png}


This assumes "if the optimal value Q*(s\textsuperscript{1}, a\textsuperscript{1}) of the sequence s\textsuperscript{1} at the next time-step was known for all possible actions a\textsuperscript{1}, then the optimal strategy is to select the action a\textsuperscript{1} maximising the expected value" (pg 2-3).  The idea is that the algorithm will use the Bellman Equation as an iterator to estimate the action-value function at each state.  As iterations tend towards infinity, the algorithms converge towards the optimal action-value function. 

An issue with this is that the action-value sequence is calculated separately at each instance, so the algorithms will not successfully converge towards the optimal action-value function.  To overcome this the DeepMind team used a function approximator (Neural Network in this case) Q(s, a; θ) ≈ Q*(s, a).  The neural net functions off a sequence of loss functions which is an expedited way of reaching the optimal action-value function. This approach worked very well for the DeepMind team and I believe a similar approach would significantly improve the performance of my Tetris agent.  The algorithmic details will be explained further in the Methods section.


\bigskip
\subsection*{Medium: Reinforcement Learning on Tetris \\
\footnotesize\normalfont https://medium.com/mlearning-ai/reinforcement-learning-on-tetris-707f75716c37}
\bigskip


This is an article published by Medium.com which is a Tetris bot they created with a reinforced learning algorithm.  This is very similar to my project, but differs in the algorithmic approach the user chose.  The figure below shows the authors algorithmic approach


\medskip
\begin{wrapfigure}

\includegraphics[width=0.45\textwidth]{Tet_Algorithm.jpeg}
\caption{}

\end{wrapfigure}
\bigskip


The first step the author described in their approach was to not train the computer how to move the pieces.  The movement actions of the computer {left, right, soft-drop, hard-drop, store, rotate} are not important since this is considered a trivial aspect of the game.  The difficulty comes from choosing where to place the piece rather than actually navigating it there, so to simplify the training the computers movement actions will be removed.  Instead the current piece is placed in every board position with a legal location and orientation, and then the computer looks at which board has the best result.  This makes it so the computer does not need to learn how to move the piece but can skip to learning where to place it.  This method will also be used in my model.  Such an approach does complicate the process of finding legal board states, but will significantly reduces training time and improve overall performance since there are less actions the computer needs to learn.

The inputs start as a 2D array of the current board which each element representing a Boolean value indicating if a piece is stored at that location.  The 2D array is the condensed into a 1D array by having the first 10 values of the array representing that corresponding columns height (how high the highest piece is) (See figure 4).  

\newpage
\begin{wrapfigure}

\includegraphics[scale=0.17]{Condensed_array.png}
\caption{}

\end{wrapfigure}
\bigskip


The next 10 elements represent each columns lowest hole position, and the final array element is a Boolean representing if the piece can be stored. Storing a piece can only be done once before a piece must be placed to prevent stalling by swapping between pieces indefinitely.  This array is then passed into the neural net. This approach has many aspects which would be very beneficial to my project, such as simplifying the board into heights and holes in a 1D array.  Currently there are 2\textsuperscript{210} different possible board states (not all legal ones) which the computer could have to analyze. Reducing the boards 2d array into just its important features would reduce the computing time of each board state which would have a large impact on the final results.  While I do think this approach could perform well, I prefer the approach the DeepMind article took. This model uses information such as board height and hole positions to determine a boards quality but a lot of this information seems redundant.  In some situations these additional details could be useful for training, but with limited computing power the amount of data passed is a large factor in the projects overall success. By simplifying the data the model should have an easier time training itself.


\end{document}
